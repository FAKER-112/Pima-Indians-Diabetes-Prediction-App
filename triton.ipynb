{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd3b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.5.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install triton\n",
    "!pip install transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a079686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton \n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(xp,yp,zp,n_length, blocksize: tl.constexpr):\n",
    "    pid=tl.program_id(axis=0) # program id\n",
    "    blockstart= pid* blocksize\n",
    "    offsets= blockstart+tl.arange(0,blocksize)\n",
    "    mask= offsets<n_length\n",
    "    x= tl.load(xp+ offsets, mask=mask)\n",
    "    y= tl.load(yp+offsets,mask=mask)\n",
    "    output=x+y\n",
    "    tl.store(zp+offsets)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89865e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N= 5000\n",
    "X = torch.randn(N, device='cuda')\n",
    "Y = torch.randn(N, device='cuda')\n",
    "Z = torch.empty_like(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3131456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Kernel function\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, # *Pointer* to first input vector.\n",
    "    y_ptr, # *Pointer* to second input vector.\n",
    "    z_ptr, # *Pointer* to output vector.\n",
    "    N, # Size of the vector.\n",
    "    BLOCK_SIZE: tl.constexpr, # Num elements each program uses\n",
    "    ):\n",
    "    # There are multiple 'programs' processing different data.\n",
    "    # We identify which program we are here:\n",
    "    pid = tl.program_id(axis=0)\n",
    "    # Offsets is a list of which elements this program instance will act on\n",
    "    # e.g. if BLOCK_SIZE is 32 these would be\n",
    "    # [0:32], [32:64], [64:96] etc, using the `pid` to find the starting index\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # Create a mask to guard memory operations against out-of-bounds acces\n",
    "    mask = offsets < N\n",
    "    # Load x and y, using the mask\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    z = x + y\n",
    "    # Write z back to HBM.\n",
    "    tl.store(z_ptr + offsets, z, mask=mask)\n",
    "\n",
    "# block_size = 1024\n",
    "# grid = ( (N + block_size - 1) // block_size, )\n",
    "# add_kernel[grid](X, Y, Z, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db2bd814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8613, -1.0471,  1.7494,  ..., -2.0297, -0.6413, -1.5074],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    # Preallocate the output.\n",
    "    z = torch.empty_like(x)\n",
    "    N = z.numel()\n",
    "    # grid can be a static tuple, or a callable that returns a tuple\n",
    "    # here it will be (N//BLOCK_SIZE,)\n",
    "    grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']), )\n",
    "    add_kernel[grid](x, y, z, N, BLOCK_SIZE=1024)\n",
    "    return z\n",
    "add(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0580bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_diffusion(x, basis, mass, evalues, times):\n",
    "    '''\n",
    "    x: (NUM_VERTS, 256)\n",
    "    basis: (NUM_VERTS, 128)\n",
    "    mass: (NUM_VERTS)\n",
    "    evalues: (128)\n",
    "    times: (256)\n",
    "    '''\n",
    "    b_t = basis.transpose(-2, -1)\n",
    "    x_m = x*mass.unsqueeze(-1)\n",
    "    in_basis = torch.matmul(b_t, x_m)\n",
    "    diffs = torch.exp(-evalues.unsqueeze(-1)*times)\n",
    "    spectral = diffs*in_basis\n",
    "    return torch.matmul(basis, spectral), spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46290194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch eager time: 0.0012359619140625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "N = 10_000_000\n",
    "A = torch.randn(N, device='cuda')\n",
    "B = torch.randn(N, device='cuda')\n",
    "C = torch.randn(N, device='cuda')\n",
    "\n",
    "# Eager PyTorch\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "\n",
    "Temp = A + B          # First trip: read A, B; write Temp\n",
    "Result = Temp * C     # Second trip: read Temp, C; write Result\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(\"PyTorch eager time:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7d8c565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton fused time: 0.004389286041259766\n"
     ]
    }
   ],
   "source": [
    "BLOCK_SIZE= 1024\n",
    "@triton.jit\n",
    "def fused_ad_mul_kernel(a_p,b_p,c_p, r_p,n:tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
    "    pid= tl.program_id(0)\n",
    "    idx= pid*BLOCK_SIZE + tl.arange(0,BLOCK_SIZE)\n",
    "    mask= idx<n \n",
    "    a= tl.load(a_p+idx, mask= mask)\n",
    "    b= tl.load(b_p+idx, mask= mask)\n",
    "    c=tl.load(c_p+idx, mask= mask)\n",
    "    r= (a+b)*c \n",
    "    tl.store(r_p+idx, r, mask=mask)\n",
    "Result_triton = torch.empty_like(A)\n",
    "\n",
    "grid = ( (N + BLOCK_SIZE - 1) // BLOCK_SIZE, )\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.time()\n",
    "fused_ad_mul_kernel[grid](A, B, C, Result_triton, N, BLOCK_SIZE)\n",
    "torch.cuda.synchronize()\n",
    "print(\"Triton fused time:\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7da885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d7048c2",
   "metadata": {},
   "source": [
    "Absolutely â€” if your goal is to build a **full Triton-based GPU layer library**, you want layers where **memory access patterns dominate performance**, or where **fusing operations can reduce trips to memory**. Hereâ€™s a structured list:\n",
    "\n",
    "---\n",
    "\n",
    "## **1ï¸âƒ£ Element-wise / simple arithmetic layers**\n",
    "\n",
    "* **Add, Subtract, Multiply, Divide** â€” useful to fuse `(A+B)*C` style ops\n",
    "* **ReLU, LeakyReLU, GELU, Sigmoid, Tanh** â€” nonlinear activations\n",
    "* **LayerNorm, BatchNorm** (forward + backward) â€” classic for transformers & CNNs\n",
    "* **Dropout** â€” can fuse with activations\n",
    "\n",
    "âœ… These are usually â€œembarrassingly parallelâ€ and easy to implement in Triton.\n",
    "\n",
    "---\n",
    "\n",
    "## **2ï¸âƒ£ Reduction-heavy layers**\n",
    "\n",
    "* **Softmax / LogSoftmax** â€” requires sum/max across one axis; ideal for deterministic accumulation\n",
    "* **Sum / Mean / Variance / Standard Deviation**\n",
    "* **Argmax / Max / Min / Top-K**\n",
    "\n",
    "ðŸ’¡ Triton allows **parallel reduction + shared memory tiling**, which is much faster than sequential loops.\n",
    "\n",
    "---\n",
    "\n",
    "## **3ï¸âƒ£ Convolutional / spatial layers**\n",
    "\n",
    "* **1D, 2D, 3D Convolutions** (forward + backward) â€” huge win when fusing tiling + memory reuse\n",
    "* **Depthwise Convolutions**\n",
    "* **Transpose Convolutions / Deconvolutions**\n",
    "* **MaxPool / AvgPool / Adaptive Pooling** (forward + backward)\n",
    "\n",
    "Triton can **tile input + kernel into shared memory**, compute multiple outputs per thread block, then write back.\n",
    "\n",
    "---\n",
    "\n",
    "## **4ï¸âƒ£ Attention / transformer layers**\n",
    "\n",
    "* **Scaled Dot-Product Attention**\n",
    "* **Multi-Head Attention** (fuse Q*K^T*V + softmax + dropout)\n",
    "* **Causal attention / masked attention**\n",
    "\n",
    "ðŸ’¡ This is where Triton **really shines**, because PyTorch usually launches multiple kernels for Q*K, softmax, dropout, and matmul, but a fused Triton kernel can do all at once in SRAM.\n",
    "\n",
    "---\n",
    "\n",
    "## **5ï¸âƒ£ Linear / fully connected layers**\n",
    "\n",
    "* **Linear / Dense layers** (matrix multiply)\n",
    "* **Batched Linear layers** (for transformers or multi-head projections)\n",
    "\n",
    "Triton can implement **custom GEMM** with tiling and shared memory reuse â€” can outperform PyTorch for special shapes or fused layers.\n",
    "\n",
    "---\n",
    "\n",
    "## **6ï¸âƒ£ Normalization and regularization layers**\n",
    "\n",
    "* **LayerNorm / RMSNorm**\n",
    "* **BatchNorm**\n",
    "* **Weight normalization**\n",
    "* **Dropout (fused with activation or linear layer)**\n",
    "\n",
    "âœ… Triton allows **forward + backward fusion**, which reduces memory writes.\n",
    "\n",
    "---\n",
    "\n",
    "## **7ï¸âƒ£ Advanced / niche layers**\n",
    "\n",
    "* **Positional encoding / Rotary embeddings**\n",
    "* **Gated activations (GLU, SwiGLU)**\n",
    "* **Fused MLP blocks** (Linear â†’ Activation â†’ Dropout â†’ Linear)\n",
    "* **Elementwise attention bias addition**\n",
    "\n",
    "ðŸ’¡ These are layers where **fusing multiple small ops** can drastically reduce memory traffic.\n",
    "\n",
    "---\n",
    "\n",
    "### **Strategy for building a full Triton library**\n",
    "\n",
    "1. **Start simple:** element-wise, activation, reductions.\n",
    "2. **Move to CNN layers:** conv, pooling, normalization.\n",
    "3. **Transformers / attention:** Q*K*V fusion, softmax, dropout.\n",
    "4. **Linear / MLP blocks:** fuse multiple linear + activation + dropout.\n",
    "5. **Add helpers/utilities:** grid/block tuning, deterministic accumulation, mixed precision support.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can make a **ready-to-go roadmap with ~20+ layers prioritized by ease â†’ performance gain**, so you can systematically rewrite kernels and build your Triton library.\n",
    "\n",
    "Do you want me to do that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de1d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AddLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: list of tensors\n",
    "        if not inputs:\n",
    "            return None\n",
    "        output = inputs[0]\n",
    "        for i in range(1, len(inputs)):\n",
    "            output = torch.add(output, inputs[i])\n",
    "        return output\n",
    "\n",
    "class SubtractLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # expects exactly 2 inputs\n",
    "        return torch.sub(inputs[0], inputs[1])\n",
    "\n",
    "class MultiplyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if not inputs:\n",
    "            return None\n",
    "        output = inputs[0]\n",
    "        for i in range(1, len(inputs)):\n",
    "            output = torch.mul(output, inputs[i])\n",
    "        return output\n",
    "\n",
    "class DivideLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # expects exactly 2 inputs\n",
    "        return torch.div(inputs[0], inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # Load and compute\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "@triton.jit\n",
    "def sub_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x - y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "@triton.jit\n",
    "def mul_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x * y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "@triton.jit\n",
    "def div_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x / y\n",
    "    tl.store(output_ptr + offsets, output, mask=mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb4c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
